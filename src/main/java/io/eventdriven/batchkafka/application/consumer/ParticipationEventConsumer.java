package io.eventdriven.batchkafka.application.consumer;

import tools.jackson.core.JsonProcessingException;
import tools.jackson.databind.json.JsonMapper;
import io.eventdriven.batchkafka.api.exception.business.CampaignNotFoundException;
import io.eventdriven.batchkafka.application.event.ParticipationEvent;
import io.eventdriven.batchkafka.application.service.ProcessingLogService;
import io.eventdriven.batchkafka.domain.entity.Campaign;
import io.eventdriven.batchkafka.domain.entity.ParticipationHistory;
import io.eventdriven.batchkafka.domain.entity.ParticipationStatus;
import io.eventdriven.batchkafka.domain.repository.CampaignRepository;
import io.eventdriven.batchkafka.domain.repository.ParticipationHistoryRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Component;
import org.springframework.transaction.annotation.Transactional;

import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

/**
 * ì„ ì°©ìˆœ ì°¸ì—¬ ì´ë²¤íŠ¸ Consumer (ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹)
 */
@Slf4j
@Component
@RequiredArgsConstructor
public class ParticipationEventConsumer {

    private final JsonMapper jsonMapper;
    private final CampaignRepository campaignRepository;
    private final ParticipationHistoryRepository participationHistoryRepository;
    private final KafkaTemplate<String, String> kafkaTemplate;
    private final ProcessingLogService processingLogService;

    private static final String DLQ_TOPIC = "campaign-participation-topic.dlq";
    private static final int LOG_INTERVAL = 1000; // 1000ê±´ë§ˆë‹¤ ë¡œê·¸

    // ì²˜ë¦¬ ê±´ìˆ˜ ì¹´ìš´í„° (ë©”ëª¨ë¦¬ ê¸°ë°˜, ì¬ì‹œì‘ ì‹œ ì´ˆê¸°í™”)
    private long processedCount = 0;
    private long successCount = 0;
    private long failCount = 0;

    @KafkaListener(
            topics = "campaign-participation-topic",
            groupId = "campaign-participation-group",
            containerFactory = "kafkaListenerContainerFactory"
    )
    @Transactional
    public void consumeParticipationEvent(List<ConsumerRecord<String, String>> records, Acknowledgment acknowledgment) {
        log.info("ğŸ“¨ Kafka ë°°ì¹˜ ìˆ˜ì‹ . ì‚¬ì´ì¦ˆ: {}ê±´", records.size());

        try {
            for (ConsumerRecord<String, String> record : records) {
                processRecord(record);
            }
            acknowledgment.acknowledge();
            log.info("âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ë° ì»¤ë°‹. ì‚¬ì´ì¦ˆ: {}ê±´", records.size());

        } catch (Exception e) {
            // @Transactionalì— ì˜í•´ ë¡¤ë°±ë˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” DLQ ì „ì†¡ ë° ì›ë³¸ ë©”ì‹œì§€ ì»¤ë°‹ë§Œ ì²˜ë¦¬
            log.error("ğŸš¨ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ. ë°°ì¹˜ ì „ì²´(ì´ {}ê±´)ë¥¼ DLQë¡œ ì „ì†¡í•©ë‹ˆë‹¤.", records.size(), e);
            sendBatchToDlq(records, "BATCH_PROCESSING_ERROR", e);
            acknowledgment.acknowledge(); // ì˜¤ë¥˜ ë°œìƒí•œ ë°°ì¹˜ëŠ” ì¬ì²˜ë¦¬í•˜ì§€ ì•Šë„ë¡ ì»¤ë°‹
        }
    }

    /**
     * ë‹¨ì¼ ë ˆì½”ë“œ ì²˜ë¦¬
     */
    private void processRecord(ConsumerRecord<String, String> record) {
        String message = record.value();
        try {
            // 1. JSON íŒŒì‹±
            ParticipationEvent event = parseMessage(message);

            // 2. Kafka ë©”íƒ€ë°ì´í„° ì„¤ì •
            event.setKafkaOffset(record.offset());
            event.setKafkaPartition(record.partition());
            event.setKafkaTimestamp(record.timestamp());

            // 3. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì‹¤í–‰
            ParticipationStatus status = processParticipation(event);

            // 4. ì¹´ìš´í„° ì—…ë°ì´íŠ¸ ë° ë¡œê¹…
            updateCountersAndLog(event, status);

        } catch (IllegalArgumentException | CampaignNotFoundException e) {
            // JSON íŒŒì‹± ì˜¤ë¥˜ ë˜ëŠ” ìº í˜ì¸ ì—†ìŒ ë“± ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ë‹¨ì¼ ë©”ì‹œì§€ ì˜¤ë¥˜
            log.error("âŒ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ë©”ì‹œì§€ ì˜¤ë¥˜ - DLQë¡œ ì „ì†¡: {}", message, e);
            sendToDlq(message, e.getClass().getSimpleName(), e);
            // ì „ì²´ ë°°ì¹˜ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•Šê³  ê³„ì† ì§„í–‰. íŠ¸ëœì­ì…˜ì€ ë¡¤ë°±ë  ê²ƒì„.
            // í•˜ì§€ë§Œ ì´ëŸ° ë©”ì‹œì§€ê°€ ìˆë‹¤ë©´ ì „ì²´ ë°°ì¹˜ê°€ ì‹¤íŒ¨í•˜ê²Œ ë˜ë¯€ë¡œ, ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì„œ ë¡¤ë°±ì„ ìœ ë„í•´ì•¼í•¨.
            throw e;
        }
        // DataAccessException ë“± ë‹¤ë¥¸ RuntimeExceptionì€ @Transactionalì— ì˜í•´ ìë™ìœ¼ë¡œ ë¡¤ë°± ì²˜ë¦¬ë¨
    }
    
    /**
     * ë©”ì‹œì§€ íŒŒì‹±
     */
    private ParticipationEvent parseMessage(String message) {
        try {
            return jsonMapper.readValue(message, ParticipationEvent.class);
        } catch (Exception e) {
            throw new IllegalArgumentException("JSON íŒŒì‹± ì‹¤íŒ¨: " + message, e);
        }
    }

    /**
     * ì°¸ì—¬ ì²˜ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
     */
    private ParticipationStatus processParticipation(ParticipationEvent event) {
        // 1. ì›ìì  ì¬ê³  ì°¨ê° (ë°˜í™˜ê°’: 0=ì¬ê³  ë¶€ì¡±, 1=ì„±ê³µ)
        int updatedRows = campaignRepository.decreaseStockAtomic(event.getCampaignId());

        ParticipationStatus status;
        if (updatedRows > 0) {
            status = ParticipationStatus.SUCCESS;
        } else {
            status = ParticipationStatus.FAIL;
        }

        // 2. ì°¸ì—¬ ì´ë ¥ ì €ì¥ (Kafka ë©”íƒ€ë°ì´í„° í¬í•¨)
        Campaign campaign = campaignRepository.findById(event.getCampaignId())
                .orElseThrow(() -> new CampaignNotFoundException(event.getCampaignId()));
        ParticipationHistory history = new ParticipationHistory(
                campaign,
                event.getUserId(),
                status,
                event.getKafkaOffset(),
                event.getKafkaPartition(),
                event.getKafkaTimestamp()
        );
        participationHistoryRepository.save(history);

        return status;
    }
    
    /**
     * ì¹´ìš´í„° ì—…ë°ì´íŠ¸ ë° 1000ê±´ë§ˆë‹¤ ë¡œê·¸
     */
    private synchronized void updateCountersAndLog(ParticipationEvent event, ParticipationStatus status) {
        processedCount++;

        if (status == ParticipationStatus.SUCCESS) {
            successCount++;
        } else {
            failCount++;
        }

        if (processedCount % LOG_INTERVAL == 0) {
            String logMessage = String.format(
                    "[Kafka Consumer] ì²˜ë¦¬ ê±´ìˆ˜: %,dê±´ | ì„±ê³µ: %,d | ì‹¤íŒ¨: %,d | ìµœê·¼ ì²˜ë¦¬: Campaign=%d, User=%d, Partition=%d, Offset=%d",
                    processedCount, successCount, failCount,
                    event.getCampaignId(), event.getUserId(),
                    event.getKafkaPartition(), event.getKafkaOffset()
            );
            processingLogService.info(logMessage);
            log.info("ğŸ“Š " + logMessage);
        }
    }

    /**
     * Dead Letter Queueë¡œ ë‹¨ì¼ ë©”ì‹œì§€ ì „ì†¡
     */
    private void sendToDlq(String originalMessage, String errorReason, Exception exception) {
        try {
            Map<String, Object> dlqMessage = new HashMap<>();
            dlqMessage.put("originalMessage", originalMessage);
            dlqMessage.put("errorReason", errorReason);
            dlqMessage.put("errorMessage", exception.getMessage());
            dlqMessage.put("errorType", exception.getClass().getSimpleName());
            dlqMessage.put("timestamp", LocalDateTime.now().toString());

            String dlqPayload = jsonMapper.writeValueAsString(dlqMessage);
            kafkaTemplate.send(DLQ_TOPIC, dlqPayload);
            log.info("ğŸ“¤ DLQ ì „ì†¡ ì™„ë£Œ - ì‚¬ìœ : {}, í† í”½: {}", errorReason, DLQ_TOPIC);
        } catch (Exception e) {
            log.error("ğŸš¨ CRITICAL: DLQ ì „ì†¡ ì‹¤íŒ¨! ì›ë³¸ ë©”ì‹œì§€: {}", originalMessage, e);
        }
    }

    /**
     * Dead Letter Queueë¡œ ë°°ì¹˜ ë©”ì‹œì§€ ì „ì†¡
     */
    private void sendBatchToDlq(List<ConsumerRecord<String, String>> records, String errorReason, Exception exception) {
        log.info("ë°°ì¹˜ DLQ ì „ì†¡ ì‹œì‘. ì´ {}ê±´", records.size());
        List<String> originalMessages = records.stream()
                                               .map(ConsumerRecord::value)
                                               .collect(Collectors.toList());
        try {
            Map<String, Object> dlqMessage = new HashMap<>();
            dlqMessage.put("originalMessages", originalMessages);
            dlqMessage.put("errorReason", errorReason);
            dlqMessage.put("errorMessage", exception.getMessage());
            dlqMessage.put("errorType", exception.getClass().getSimpleName());
            dlqMessage.put("batchSize", records.size());
            dlqMessage.put("timestamp", LocalDateTime.now().toString());

            String dlqPayload = jsonMapper.writeValueAsString(dlqMessage);
            kafkaTemplate.send(DLQ_TOPIC, dlqPayload);
            log.info("ğŸ“¤ ë°°ì¹˜ DLQ ì „ì†¡ ì™„ë£Œ - ì‚¬ìœ : {}, í† í”½: {}", errorReason, DLQ_TOPIC);
        } catch (Exception e) {
            log.error("ğŸš¨ CRITICAL: ë°°ì¹˜ DLQ ì „ì†¡ ì‹¤íŒ¨! ì „ì²´ ë©”ì‹œì§€ë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¡œê¹…í•©ë‹ˆë‹¤.", e);
            for (String msg : originalMessages) {
                log.error("ë°°ì¹˜ DLQ ì‹¤íŒ¨ ê°œë³„ ë©”ì‹œì§€: {}", msg);
            }
        }
    }
}